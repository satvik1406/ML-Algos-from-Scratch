{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ztLkn9I-MI"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randrange\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rklc1wTJrhO"
      },
      "source": [
        "df = pd.read_csv(\"dataset_LR.csv\", header=0)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzx8MJ1xL-UX"
      },
      "source": [
        "def Sigmoid(z):\n",
        "    G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))\n",
        "    return G_of_Z\n",
        "\n",
        "def Hypothesis(theta, x):\n",
        "    z = 0\n",
        "    for i in range(len(theta)):\n",
        "        z += x[i]*theta[i]\n",
        "    return Sigmoid(z)\n",
        "\n",
        "def Cost_Function(X,Y,theta,m):\n",
        "    sumOfErrors = 0\n",
        "    for i in range(m):\n",
        "        xi = X[i]\n",
        "        hi = Hypothesis(theta,xi)\n",
        "        if Y[i] == 1:\n",
        "            error = Y[i] * math.log(hi)\n",
        "        elif Y[i] == 0:\n",
        "            error = (1-Y[i]) * math.log(1-hi)\n",
        "        sumOfErrors += error\n",
        "    const = -1/m\n",
        "    J = const * sumOfErrors\n",
        "    # print ('cost is ', J )\n",
        "    return J\n",
        "\n",
        "def Cost_Function_Derivative(X,Y,theta,j,m,alpha):\n",
        "    sumErrors = 0\n",
        "    for i in range(m):\n",
        "        xi = X[i]\n",
        "        xij = xi[j]\n",
        "        hi = Hypothesis(theta,X[i])\n",
        "        error = (hi - Y[i])*xij\n",
        "        sumErrors += error\n",
        "    m = len(Y)\n",
        "    constant = float(alpha)/float(m)\n",
        "    J = constant * sumErrors\n",
        "    return J\n",
        "\n",
        "def Gradient_Descent(X,Y,theta,m,alpha):\n",
        "    new_theta = []\n",
        "    constant = alpha/m\n",
        "    for j in range(len(theta)):\n",
        "        CFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)\n",
        "        #print(theta[j])\n",
        "        new_theta_value = theta[j] - CFDerivative\n",
        "        new_theta.append(new_theta_value)\n",
        "    return new_theta\n",
        "\n",
        "def Stoc_Cost_Function_Derivative(X,Y,theta,j,m,alpha):\n",
        "    sumErrors = 0\n",
        "    i = np.random.randint(len(X))\n",
        "    xi = X[i]\n",
        "    xij = xi[j]\n",
        "    hi = Hypothesis(theta,X[i])\n",
        "    error = (hi - Y[i])*xij\n",
        "    sumErrors += error\n",
        "    m = len(Y)\n",
        "    constant = float(alpha)/float(m)\n",
        "    J = constant * sumErrors\n",
        "    return J\n",
        "\n",
        "def Stochastic_Gradient_Descent(X,Y,theta,m,alpha):\n",
        "    # print(\"Entered\")\n",
        "    new_theta = []\n",
        "    constant = alpha/m\n",
        "    for j in range(len(theta)):\n",
        "      CFDerivative = Stoc_Cost_Function_Derivative(X,Y,theta,j,m,alpha)\n",
        "        #print(theta[j])\n",
        "      new_theta_value = theta[j] - CFDerivative\n",
        "      new_theta.append(new_theta_value)\n",
        "    return new_theta\n",
        "\n",
        "def Declare_Winner(theta,X_test,Y_test):\n",
        "    predict = list()\n",
        "    score = 0\n",
        "    length = len(X_test)\n",
        "    for i in range(length):\n",
        "        predict.append(round(Hypothesis(X_test[i],theta)))\n",
        "        prediction = round(Hypothesis(X_test[i],theta))\n",
        "        answer = Y_test[i]\n",
        "        if prediction == answer:\n",
        "            score += 1\n",
        "    my_score = float(score) / float(length)\n",
        "    return my_score,predict\n",
        "\n",
        "def Logistic_Regression(X,Y,alpha,theta,num_iters,model):\n",
        "    m = len(Y)\n",
        "    acc_epoch = list()\n",
        "    cost_epoch = list()\n",
        "    for x in range(num_iters):\n",
        "        if model == 1: \n",
        "          new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
        "        elif model == 2:\n",
        "            new_theta = Stochastic_Gradient_Descent(X,Y,theta,m,alpha)\n",
        "        theta = new_theta\n",
        "        if x % 50 == 0:\n",
        "            #here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration\n",
        "            cost = Cost_Function(X,Y,theta,m)\n",
        "            # cost_epoch.append(cost)\n",
        "            # acc,xyz = Declare_Winner(theta,X,Y)\n",
        "            # acc_epoch.append(acc)\n",
        "            # #print ('theta ', theta)\n",
        "            #print ('cost is ', Cost_Function(X,Y,theta,m))\n",
        "    # return theta,acc_epoch,cost_epoch\n",
        "    return theta,cost"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9VL-nJ7LR2Y"
      },
      "source": [
        "def f_score_metric(actual,predicted):\n",
        "\ttrue_pos=0\n",
        "\ttrue_neg=0\n",
        "\tfalse_pos=0\n",
        "\tfalse_neg=0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i] and predicted[i]==1:#true pos\n",
        "\t\t\ttrue_pos += 1\n",
        "\t\telif actual[i] == predicted[i] and predicted[i]==0:#true neg\n",
        "\t\t\ttrue_neg += 1\n",
        "\t\telif actual[i] ==0 and predicted[i]==1:#false pos\n",
        "\t\t\tfalse_pos+= 1\n",
        "\t\telse:\n",
        "\t\t\tfalse_neg+=1\n",
        "\tprecision =true_pos/(true_pos+false_pos)\n",
        "\trecall = true_pos/(true_pos+false_neg)\n",
        "\tf_score=2*precision*recall/(precision+recall)\n",
        "\treturn f_score,precision,recall"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otPO5jS7LFNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73076ab-a163-49ec-f82a-a4d2f3a49cb8"
      },
      "source": [
        "accuracies_test = list()\n",
        "f_scores_test = list()\n",
        "recalls_test =list()\n",
        "precisions_test = list()\n",
        "accuracies_train = list()\n",
        "f_scores_train = list()\n",
        "recalls_train =list()\n",
        "precisions_train = list()\n",
        "costs = list()\n",
        "iters = list()\n",
        "iterations = 5000\n",
        "for i in range(iterations):\n",
        "  if i%50 == 0:\n",
        "    iters.append(i)\n",
        "df.columns = [\"attr1\",\"attr2\",\"attr3\",\"attr4\",\"class\"]\n",
        "for i in range(10):\n",
        "  train_data = df.sample(frac=0.7,random_state=(np.random.randint(1,1000,1))[0])\n",
        "  test_data = df.drop(train_data.index)\n",
        "  X_train = train_data[[\"attr1\",\"attr2\",\"attr3\",\"attr4\"]]\n",
        "  X_test = test_data[[\"attr1\",\"attr2\",\"attr3\",\"attr4\"]]\n",
        "  Y_train = train_data[\"class\"]\n",
        "  Y_test = test_data[\"class\"]\n",
        "  X_train = np.array(X_train)\n",
        "  X_test = np.array(X_test)\n",
        "  Y_train = np.array(Y_train)\n",
        "  Y_test = np.array(Y_test)\n",
        "  # print(X_train)\n",
        "  # print(Y_train)\n",
        "  initial_theta = [0,0,0,0]\n",
        "  alpha = 0.1\n",
        "  # predicted,acc_epoch,cost_epoch = Logistic_Regression(X_train,Y_train,alpha,initial_theta,iterations,1)\n",
        "  predicted,cost = Logistic_Regression(X_train,Y_train,alpha,initial_theta,iterations,1)\n",
        "  print(predicted)\n",
        "  costs.append(cost)\n",
        "  accuracy_train,predict_train = Declare_Winner(predicted,X_train,Y_train)\n",
        "  f_score_train,precision_train,recall_train = f_score_metric(Y_train,predict_train)\n",
        "  f_scores_train.append(f_score_train)\n",
        "  precisions_train.append(precision_train)\n",
        "  recalls_train.append(recall_train)\n",
        "  accuracies_train.append(accuracy_train*100)\n",
        "  accuracy_test,predict_test = Declare_Winner(predicted,X_test,Y_test)\n",
        "  f_score_test,precision_test,recall_test = f_score_metric(Y_test,predict_test)\n",
        "  f_scores_test.append(f_score_test)\n",
        "  precisions_test.append(precision_test)\n",
        "  recalls_test.append(recall_test)\n",
        "  accuracies_test.append(accuracy_test*100)\n",
        "\n",
        "# print('accuracies 50 iteration %s'% acc_epoch)\n",
        "# print('cost 50 iteration %s'% cost_epoch) \n",
        "print('____TRAIN DATA____') \n",
        "print('Accuracies over each iteration: %s' % accuracies_train)\n",
        "print('Precisions over each iteration: %s' % precisions_train)\n",
        "print('Recalls over each iteration: %s' % recalls_train)\n",
        "print('Average Accuracy: %.3f' % (sum(accuracies_train)/float(len(accuracies_train))))\n",
        "print('Average Cost: %.3f' % (sum(costs)/float(len(costs))))\n",
        "print('____TEST DATA____') \n",
        "print('Accuracies over each iteration: %s' % accuracies_test)\n",
        "print('Precisions over each iteration: %s' % precisions_test)\n",
        "print('Recalls over each iteration: %s' % recalls_test)\n",
        "print('Average Accuracy: %.3f' % (sum(accuracies_test)/float(len(accuracies_test))))\n",
        "\n",
        "# plt.plot(iters,acc_epoch)\n",
        "# plt.xlabel(\"Epoches\")\n",
        "# plt.ylabel(\"Accuracies\")\n",
        "# plt.title(\"Accuracies vs Epoches\")\n",
        "# plt.show()\n",
        "# # [0.2,] [95.955,]\n",
        "# plt.plot(iters,cost_epoch)\n",
        "# plt.xlabel(\"Epoches\")\n",
        "# plt.ylabel(\"Cost\")\n",
        "# plt.title(\"Cost vs Epoches\")\n",
        "# plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.5053395011441206, -1.5859682701822717, -1.6346605115341555, -0.8388766163489728]\n",
            "[-2.704049604206519, -1.683888043377677, -1.7638308643054328, -0.8467033073400532]\n",
            "[-2.6526237059416333, -1.6181826054762563, -1.7271110903989826, -0.8950377356359108]\n",
            "[-2.524579160389126, -1.5631203616973501, -1.6466537279776574, -0.7626993925337276]\n",
            "[-2.722970481588964, -1.6885775231958045, -1.7752314676777556, -0.9238333671031972]\n",
            "[-2.8958976783612576, -1.7385664316342644, -1.8496177662312232, -0.7900708434591098]\n",
            "[-2.45459460653867, -1.5217658499684765, -1.6160546448458375, -0.7481905270663857]\n",
            "[-2.589987756938105, -1.6873025162930757, -1.7750083464320852, -0.8168952518146475]\n",
            "[-2.704034096737561, -1.5961025801815683, -1.6847759903775472, -0.7462002326631328]\n",
            "[-2.6336624084455997, -1.6580176602160357, -1.7612392389452245, -0.8039344817461458]\n",
            "____TRAIN DATA____\n",
            "Accuracies over each iteration: [95.83333333333334, 95.9375, 96.04166666666667, 95.3125, 96.04166666666667, 95.9375, 95.0, 96.04166666666667, 95.72916666666667, 95.625]\n",
            "Precisions over each iteration: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Recalls over each iteration: [0.9071925754060325, 0.9088785046728972, 0.9075425790754258, 0.8960739030023095, 0.9107981220657277, 0.9093023255813953, 0.8865248226950354, 0.9086538461538461, 0.9026128266033254, 0.9011764705882352]\n",
            "Average Accuracy: 95.750\n",
            "Average Cost: 0.099\n",
            "____TEST DATA____\n",
            "Accuracies over each iteration: [95.87378640776699, 95.63106796116504, 95.3883495145631, 97.0873786407767, 95.63106796116504, 95.63106796116504, 97.81553398058253, 95.14563106796116, 96.35922330097088, 96.60194174757282]\n",
            "Precisions over each iteration: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Recalls over each iteration: [0.9050279329608939, 0.9010989010989011, 0.9045226130653267, 0.9322033898305084, 0.9021739130434783, 0.9, 0.9518716577540107, 0.8969072164948454, 0.9206349206349206, 0.9243243243243243]\n",
            "Average Accuracy: 96.117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INZ-mQfWQDEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f9e952-3830-4cb8-a898-a07dde57e797"
      },
      "source": [
        "accuracies_test = list()\n",
        "f_scores_test = list()\n",
        "recalls_test =list()\n",
        "precisions_test = list()\n",
        "accuracies_train = list()\n",
        "f_scores_train = list()\n",
        "recalls_train =list()\n",
        "costs = list()\n",
        "precisions_train = list()\n",
        "iterations = 7500\n",
        "iters = list()\n",
        "for i in range(iterations):\n",
        "  if i%50 == 0:\n",
        "    iters.append(i)\n",
        "\n",
        "df.columns = [\"attr1\",\"attr2\",\"attr3\",\"attr4\",\"class\"]\n",
        "for i in range(10):\n",
        "  train_data = df.sample(frac=0.7,random_state=(np.random.randint(1,1000,1))[0])\n",
        "  test_data = df.drop(train_data.index)\n",
        "  X_train = train_data[[\"attr1\",\"attr2\",\"attr3\",\"attr4\"]]\n",
        "  X_test = test_data[[\"attr1\",\"attr2\",\"attr3\",\"attr4\"]]\n",
        "  Y_train = train_data[\"class\"]\n",
        "  Y_test = test_data[\"class\"]\n",
        "  X_train = np.array(X_train)\n",
        "  X_test = np.array(X_test)\n",
        "  Y_train = np.array(Y_train)\n",
        "  Y_test = np.array(Y_test)\n",
        "  # print(X_train)\n",
        "  # print(Y_train)\n",
        "  initial_theta = [0,0,0,0]\n",
        "  alpha = 0.9\n",
        "  # predicted,acc_epoch,cost_epoch = Logistic_Regression(X_train,Y_train,alpha,initial_theta,iterations,2)\n",
        "  predicted,cost = Logistic_Regression(X_train,Y_train,alpha,initial_theta,iterations,2)\n",
        "  print(predicted)\n",
        "  costs.append(cost)\n",
        "  accuracy_train,predict_train = Declare_Winner(predicted,X_train,Y_train)\n",
        "  f_score_train,precision_train,recall_train = f_score_metric(Y_train,predict_train)\n",
        "  f_scores_train.append(f_score_train)\n",
        "  precisions_train.append(precision_train)\n",
        "  recalls_train.append(recall_train)\n",
        "  accuracies_train.append(accuracy_train*100)\n",
        "  accuracy_test,predict_test = Declare_Winner(predicted,X_test,Y_test)\n",
        "  f_score_test,precision_test,recall_test = f_score_metric(Y_test,predict_test)\n",
        "  f_scores_test.append(f_score_test)\n",
        "  precisions_test.append(precision_test)\n",
        "  recalls_test.append(recall_test)\n",
        "  accuracies_test.append(accuracy_test*100)\n",
        "  \n",
        "\n",
        "# print('accuracies 50 iteration %s'% acc_epoch)\n",
        "# print('cost 50 iteration %s'% cost_epoch)  \n",
        "print('____TRAIN DATA____') \n",
        "print('Accuracies over each iteration: %s' % accuracies_train)\n",
        "print('Precisions over each iteration: %s' % precisions_train)\n",
        "print('Recalls over each iteration: %s' % recalls_train)\n",
        "print('Average Accuracy: %.3f' % (sum(accuracies_train)/float(len(accuracies_train))))\n",
        "print('Average Cost: %.3f' % (sum(costs)/float(len(costs))))\n",
        "print('____TEST DATA____') \n",
        "print('Accuracies over each iteration: %s' % accuracies_test)\n",
        "print('Precisions over each iteration: %s' % precisions_test)\n",
        "print('Recalls over each iteration: %s' % recalls_test)\n",
        "print('Average Accuracy: %.3f' % (sum(accuracies_test)/float(len(accuracies_test))))\n",
        "\n",
        "# plt.plot(iters,acc_epoch)\n",
        "# plt.xlabel(\"Epoches\")\n",
        "# plt.ylabel(\"Accuracies\")\n",
        "# plt.title(\"Accuracies vs Epoches\")\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(iters,cost_epoch)\n",
        "# plt.xlabel(\"Epoches\")\n",
        "# plt.ylabel(\"Cost\")\n",
        "# plt.title(\"Cost vs Epoches\")\n",
        "# plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.09359777659756, -0.5494752390214777, -0.543920584722556, -0.30457573398466076]\n",
            "[-1.0710210380749527, -0.528814934452572, -0.4997054546712983, -0.2878070625031467]\n",
            "[-1.077726338612576, -0.5572348452348411, -0.509781174466149, -0.31663463429538896]\n",
            "[-1.0036077279114015, -0.5320222790995524, -0.48190764735818, -0.3041124582625317]\n",
            "[-1.0917563399312118, -0.5568593475262846, -0.5406071599122001, -0.3327525418414671]\n",
            "[-1.1181635463575674, -0.5580775086938273, -0.5434280932957295, -0.28700128979609957]\n",
            "[-1.1212129413772254, -0.5236994901895218, -0.5452486724694245, -0.3068212307910015]\n",
            "[-1.058791439785771, -0.5575179273747184, -0.5155531951420471, -0.31960944188470997]\n",
            "[-1.060837382261309, -0.5713716856333497, -0.5431167848399537, -0.2964942383857477]\n",
            "[-1.0957980531556661, -0.5354275497449095, -0.5593892385923712, -0.29521189301530915]\n",
            "____TRAIN DATA____\n",
            "Accuracies over each iteration: [95.52083333333333, 95.3125, 95.0, 94.27083333333334, 95.3125, 95.20833333333333, 95.10416666666667, 94.6875, 94.6875, 95.625]\n",
            "Precisions over each iteration: [1.0, 0.9895287958115183, 0.9950617283950617, 0.9794871794871794, 0.9947643979057592, 1.0, 0.9892761394101877, 0.9923273657289002, 0.9923076923076923, 0.9944598337950139]\n",
            "Recalls over each iteration: [0.9029345372460497, 0.9021479713603818, 0.8975501113585747, 0.8904428904428905, 0.8983451536643026, 0.892018779342723, 0.8956310679611651, 0.8899082568807339, 0.8896551724137931, 0.899749373433584]\n",
            "Average Accuracy: 95.073\n",
            "Average Cost: 0.145\n",
            "____TEST DATA____\n",
            "Accuracies over each iteration: [95.63106796116504, 94.66019417475728, 95.14563106796116, 95.63106796116504, 95.3883495145631, 95.87378640776699, 94.66019417475728, 96.35922330097088, 96.60194174757282, 94.41747572815534]\n",
            "Precisions over each iteration: [1.0, 0.9941520467836257, 0.9795918367346939, 0.9822485207100592, 0.9941176470588236, 1.0, 0.9888888888888889, 0.9877300613496932, 0.9878787878787879, 0.9947368421052631]\n",
            "Recalls over each iteration: [0.8922155688622755, 0.8900523560209425, 0.8944099378881988, 0.9171270718232044, 0.9037433155080213, 0.907608695652174, 0.898989898989899, 0.9252873563218391, 0.9314285714285714, 0.8957345971563981]\n",
            "Average Accuracy: 95.437\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}